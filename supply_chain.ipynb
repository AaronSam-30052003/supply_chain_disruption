{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dSmbyvWKQ-w6EqkZ5rKPqFna-xOzUWW2",
      "authorship_tag": "ABX9TyN6SDNt90Ks3+xm42wWU+Wu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronSam-30052003/supply_chain_disruption/blob/main/supply_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "govjJ9HzOEXG",
        "outputId": "417e5c12-af3a-49eb-ac3d-5cc0f580aa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 Title  \\\n",
            "0  World Economic Outlook - All Issues   \n",
            "\n",
            "                                         Description                   URL  \\\n",
            "0  The World Economic Outlook (WEO) is a survey o...  /ar/Publications/WEO   \n",
            "\n",
            "    Source Published At  \n",
            "0  Unknown      Unknown  \n",
            "Data saved to supply_chain_impact_news.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.imf.org/en/publications/weo'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "articles = []\n",
        "\n",
        "count = 0\n",
        "for article in soup.find_all('article'):\n",
        "    if count >= 50:\n",
        "        break\n",
        "\n",
        "    title = article.find('h2')\n",
        "    description = article.find('p')\n",
        "    link = article.find('a', href=True)\n",
        "    source = article.find('span', class_='source')\n",
        "    published = article.find('time')\n",
        "\n",
        "    if title and description and link:\n",
        "        articles.append({\n",
        "            'Title': title.get_text(),\n",
        "            'Description': description.get_text(),\n",
        "            'URL': link['href'],\n",
        "            'Source': source.get_text() if source else 'Unknown',\n",
        "            'Published At': published.get_text() if published else 'Unknown'\n",
        "        })\n",
        "        count += 1\n",
        "\n",
        "news_df = pd.DataFrame(articles)\n",
        "\n",
        "print(news_df.head())\n",
        "\n",
        "csv_file_path = \"supply_chain_impact_news.csv\"\n",
        "try:\n",
        "    news_df.to_csv(csv_file_path, index=False)\n",
        "    print(f\"Data saved to {csv_file_path}\")\n",
        "except IOError as e:\n",
        "    print(f\"Error saving data to CSV: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY_MEDIASTACK = '939de3f5d7bcd9030de41c23754ec20b'\n",
        "BASE_URL_MEDIASTACK = 'https://api.mediastack.com/v1/news'\n",
        "API_KEY_GOOGLE = \"AIzaSyBRu1jSldkSrJbWXcHPOWfy7s1K_TlvV54\"\n",
        "CSE_ID = \"d03a2f288016547d9\"\n",
        "API_KEY_NEWSAPI = \"271b4406b0f24a899c09bb77c3723780\"\n",
        "\n",
        "\n",
        "def fetch_media_stack_articles():\n",
        "    params = {\n",
        "        'access_key': API_KEY_MEDIASTACK,\n",
        "        'keywords': 'transportation',\n",
        "        'category': 'business'\n",
        "    }\n",
        "    response = requests.get(BASE_URL_MEDIASTACK, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def filter_mediastack_articles(data):\n",
        "    if data and 'data' in data:\n",
        "        filtered_articles = [\n",
        "            {\n",
        "                'source': article.get('source'),\n",
        "                'publisher': article.get('author'),\n",
        "                'title': article.get('title'),\n",
        "                'content': article.get('description'),\n",
        "                'language': article.get('language'),\n",
        "                'date': article.get('published_at')\n",
        "            }\n",
        "            for article in data['data']\n",
        "        ]\n",
        "        return filtered_articles\n",
        "    else:\n",
        "        print(\"No articles retrieved.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def search_engine_news(query, num_results=100):\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    results = []\n",
        "    start_index = 1\n",
        "    while len(results) < num_results:\n",
        "        params = {\n",
        "            \"key\": API_KEY_GOOGLE,\n",
        "            \"cx\": CSE_ID,\n",
        "            \"q\": query,\n",
        "            \"num\": 10,\n",
        "            \"start\": start_index,\n",
        "            \"sort\": \"date\"\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            items = data.get(\"items\", [])\n",
        "            if not items:\n",
        "                break\n",
        "            results.extend(items)\n",
        "            start_index += 10\n",
        "            time.sleep(1)\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "            break\n",
        "    return results[:num_results]\n",
        "\n",
        "\n",
        "def filter_search_engine_news(results):\n",
        "    filtered_results = []\n",
        "    for item in results:\n",
        "        title = item.get(\"title\", \"No title\")\n",
        "        link = item.get(\"link\", \"No link\")\n",
        "        snippet = item.get(\"snippet\", \"No content\")\n",
        "        publisher = item.get(\"displayLink\", \"Unknown publisher\")\n",
        "        filtered_results.append({\n",
        "            \"title\": title,\n",
        "            \"url\": link,\n",
        "            \"content\": snippet,\n",
        "            \"publisher\": publisher,\n",
        "            \"source\": publisher,\n",
        "            \"date\": \"Unknown date\"\n",
        "        })\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "def fetch_newsapi_articles(api_key, query, page_size=100, max_results=100):\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    all_articles = []\n",
        "    page = 1\n",
        "    while len(all_articles) < max_results:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"apiKey\": api_key\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            articles = data.get('articles', [])\n",
        "            if not articles:\n",
        "                break\n",
        "            all_articles.extend(articles)\n",
        "            page += 1\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            break\n",
        "    return all_articles\n",
        "\n",
        "\n",
        "media_stack_data = fetch_media_stack_articles()\n",
        "media_stack = filter_mediastack_articles(media_stack_data)\n",
        "if media_stack:\n",
        "    df_media_stack = pd.DataFrame(media_stack)\n",
        "    df_media_stack.to_csv(\"media_stack_filtered_supply_chain_articles.csv\", index=False)\n",
        "\n",
        "query = \"shipment delay\"\n",
        "results = search_engine_news(query)\n",
        "filtered_results = filter_search_engine_news(results)\n",
        "if filtered_results:\n",
        "    df_google = pd.DataFrame(filtered_results)\n",
        "    df_google.to_csv(\"custom_search_engine_news.csv\", index=False)\n",
        "\n",
        "query = \"disruption\"\n",
        "all_articles = fetch_newsapi_articles(API_KEY_NEWSAPI, query)\n",
        "if all_articles:\n",
        "    df_newsapi = pd.DataFrame(all_articles)\n",
        "    df_newsapi.drop(columns=[\"urlToImage\", \"source\"], inplace=True, errors='ignore')\n",
        "    df_newsapi.rename(columns={\"publishedAt\": \"source\"}, inplace=True)\n",
        "    df_newsapi.to_csv(\"newsapi_articles.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "lnYe0gNccACY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "API_KEY = 'a0f51944e3b96c538c646b49c12e3bc3'\n",
        "\n",
        "def fetch_weather_data(city):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    latitude, longitude = get_city_coordinates(city)\n",
        "    if not latitude or not longitude:\n",
        "        print(f\"City coordinates not found for {city}!\")\n",
        "        return\n",
        "\n",
        "    params = {\n",
        "        \"lat\": latitude,\n",
        "        \"lon\": longitude,\n",
        "        \"appid\": API_KEY,\n",
        "        \"units\": \"metric\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        weather_data = [{\n",
        "            \"city\": city,\n",
        "            \"date\": datetime.datetime.now().strftime('%Y-%m-%d'),\n",
        "            \"temperature\": data['main']['temp'],\n",
        "            \"humidity\": data['main']['humidity'],\n",
        "            \"weather\": data['weather'][0]['description']\n",
        "        }]\n",
        "        return weather_data\n",
        "    else:\n",
        "        print(f\"Error fetching current weather data for {city}: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "def get_city_coordinates(city):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\n",
        "        \"q\": city,\n",
        "        \"appid\": API_KEY\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data['coord']['lat'], data['coord']['lon']\n",
        "    else:\n",
        "        print(f\"Error fetching city coordinates for {city}: {response.status_code}, {response.text}\")\n",
        "        return None, None\n",
        "\n",
        "def save_to_csv(data, filename=\"weather_data.csv\"):\n",
        "    if not data:\n",
        "        print(\"No data to save!\")\n",
        "        return\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Weather data saved to {filename}\")\n",
        "\n",
        "cities = [\n",
        "    \"Chennai\", \"Mumbai\", \"Delhi\", \"Bengaluru\", \"Kolkata\", \"Hyderabad\", \"Ahmedabad\", \"Chandigarh\", \"Jaipur\", \"Surat\",\n",
        "    \"Pune\", \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Indore\", \"Vadodara\", \"Coimbatore\", \"Patna\", \"Agra\", \"Visakhapatnam\",\n",
        "    \"Madurai\", \"Rajkot\", \"Meerut\", \"Varanasi\", \"Srinagar\", \"Jammu\", \"Bhopal\", \"Guwahati\", \"Noida\", \"Faridabad\",\n",
        "    \"Ludhiana\", \"Mysuru\", \"Ranchi\", \"Kochi\", \"Gurugram\", \"Dhanbad\", \"Aurangabad\", \"Kochi\", \"Chandrapur\", \"Vijayawada\",\n",
        "    \"Thane\", \"Nagaland\", \"Bhubaneswar\", \"Goa\", \"Navi Mumbai\", \"Kolkata\", \"Patiala\", \"Jodhpur\", \"Mangalore\", \"Raipur\",\n",
        "    \"Udaipur\", \"Aligarh\", \"Shivpuri\"\n",
        "]\n",
        "\n",
        "all_weather_data = []\n",
        "for city in cities:\n",
        "    weather_data = fetch_weather_data(city)\n",
        "    if weather_data:\n",
        "        all_weather_data.extend(weather_data)\n",
        "save_to_csv(all_weather_data)\n"
      ],
      "metadata": {
        "id": "AXZ9gLlOUTUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cbfb02-af4c-4745-d5e8-e499681d60b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather data saved to weather_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "API_KEY = 'a0f51944e3b96c538c646b49c12e3bc3'\n",
        "\n",
        "def fetch_weather_data(city):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/forecast\"\n",
        "    latitude, longitude = get_city_coordinates(city)\n",
        "    if not latitude or not longitude:\n",
        "        print(f\"City coordinates not found for {city}!\")\n",
        "        return []\n",
        "\n",
        "    params = {\n",
        "        \"lat\": latitude,\n",
        "        \"lon\": longitude,\n",
        "        \"appid\": API_KEY,\n",
        "        \"units\": \"metric\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        weather_data = []\n",
        "        for forecast in data['list']:\n",
        "            weather_data.append({\n",
        "                \"city\": city,\n",
        "                \"date\": datetime.datetime.fromtimestamp(forecast['dt']).strftime('%Y-%m-%d'),\n",
        "                \"temperature\": forecast['main']['temp'],\n",
        "                \"humidity\": forecast['main']['humidity'],\n",
        "                \"weather\": forecast['weather'][0]['description']\n",
        "            })\n",
        "        return weather_data\n",
        "    else:\n",
        "        print(f\"Error fetching forecast data for {city}: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "def get_city_coordinates(city):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\n",
        "        \"q\": city,\n",
        "        \"appid\": API_KEY\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data['coord']['lat'], data['coord']['lon']\n",
        "    else:\n",
        "        print(f\"Error fetching city coordinates for {city}: {response.status_code}, {response.text}\")\n",
        "        return None, None\n",
        "\n",
        "def save_to_csv(data, filename=\"weather_data.csv\"):\n",
        "    if not data:\n",
        "        print(\"No data to save!\")\n",
        "        return\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Weather data saved to {filename}\")\n",
        "\n",
        "cities = [\n",
        "    \"Chennai\", \"Mumbai\", \"Delhi\", \"Bengaluru\", \"Kolkata\", \"Hyderabad\", \"Ahmedabad\", \"Chandigarh\", \"Jaipur\", \"Surat\",\n",
        "    \"Pune\", \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Indore\", \"Vadodara\", \"Coimbatore\", \"Patna\", \"Agra\", \"Visakhapatnam\",\n",
        "    \"Madurai\", \"Rajkot\", \"Meerut\", \"Varanasi\", \"Srinagar\", \"Jammu\", \"Bhopal\", \"Guwahati\", \"Noida\", \"Faridabad\",\n",
        "    \"Ludhiana\", \"Mysuru\", \"Ranchi\", \"Kochi\", \"Gurugram\", \"Dhanbad\", \"Aurangabad\", \"Vijayawada\", \"Thane\", \"Bhubaneswar\",\n",
        "    \"Goa\", \"Navi Mumbai\", \"Patiala\", \"Jodhpur\", \"Mangalore\", \"Raipur\", \"Udaipur\", \"Aligarh\", \"Shivpuri\", \"Kollam\"\n",
        "]\n",
        "\n",
        "all_weather_data = []\n",
        "\n",
        "for city in cities:\n",
        "    weather_data = fetch_weather_data(city)\n",
        "    if weather_data:\n",
        "        all_weather_data.extend(weather_data)\n",
        "\n",
        "\n",
        "save_to_csv(all_weather_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lnmhAUHcGzH",
        "outputId": "49e894bf-3e7a-42e5-9286-e70adbc8dd0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather data saved to weather_data.csv\n"
          ]
        }
      ]
    }
  ]
}