{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dSmbyvWKQ-w6EqkZ5rKPqFna-xOzUWW2",
      "authorship_tag": "ABX9TyOnNseWu6/zyXQJeRPOZJe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronSam-30052003/supply_chain_disruption/blob/main/supply_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "govjJ9HzOEXG",
        "outputId": "07cf3844-8190-4f4f-9cc9-329da59e9a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 Title  \\\n",
            "0  World Economic Outlook - All Issues   \n",
            "\n",
            "                                         Description                   URL  \\\n",
            "0  The World Economic Outlook (WEO) is a survey o...  /ar/Publications/WEO   \n",
            "\n",
            "    Source Published At  \n",
            "0  Unknown      Unknown  \n",
            "Data saved to supply_chain_impact_news.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.imf.org/en/publications/weo'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "articles = []\n",
        "\n",
        "count = 0\n",
        "for article in soup.find_all('article'):\n",
        "    if count >= 50:\n",
        "        break\n",
        "\n",
        "    title = article.find('h2')\n",
        "    description = article.find('p')\n",
        "    link = article.find('a', href=True)\n",
        "    source = article.find('span', class_='source')\n",
        "    published = article.find('time')\n",
        "\n",
        "    if title and description and link:\n",
        "        articles.append({\n",
        "            'Title': title.get_text(),\n",
        "            'Description': description.get_text(),\n",
        "            'URL': link['href'],\n",
        "            'Source': source.get_text() if source else 'Unknown',\n",
        "            'Published At': published.get_text() if published else 'Unknown'\n",
        "        })\n",
        "        count += 1\n",
        "\n",
        "news_df = pd.DataFrame(articles)\n",
        "\n",
        "print(news_df.head())\n",
        "\n",
        "csv_file_path = \"supply_chain_impact_news.csv\"\n",
        "try:\n",
        "    news_df.to_csv(csv_file_path, index=False)\n",
        "    print(f\"Data saved to {csv_file_path}\")\n",
        "except IOError as e:\n",
        "    print(f\"Error saving data to CSV: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY_MEDIASTACK = '939de3f5d7bcd9030de41c23754ec20b'\n",
        "BASE_URL_MEDIASTACK = 'https://api.mediastack.com/v1/news'\n",
        "API_KEY_GOOGLE = \"AIzaSyBRu1jSldkSrJbWXcHPOWfy7s1K_TlvV54\"\n",
        "CSE_ID = \"d03a2f288016547d9\"\n",
        "API_KEY_NEWSAPI = \"271b4406b0f24a899c09bb77c3723780\"\n",
        "\n",
        "\n",
        "def fetch_media_stack_articles():\n",
        "    params = {\n",
        "        'access_key': API_KEY_MEDIASTACK,\n",
        "        'keywords': 'transportation',\n",
        "        'category': 'business'\n",
        "    }\n",
        "    response = requests.get(BASE_URL_MEDIASTACK, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def filter_mediastack_articles(data):\n",
        "    if data and 'data' in data:\n",
        "        filtered_articles = [\n",
        "            {\n",
        "                'source': article.get('source'),\n",
        "                'publisher': article.get('author'),\n",
        "                'title': article.get('title'),\n",
        "                'content': article.get('description'),\n",
        "                'language': article.get('language'),\n",
        "                'date': article.get('published_at')\n",
        "            }\n",
        "            for article in data['data']\n",
        "        ]\n",
        "        return filtered_articles\n",
        "    else:\n",
        "        print(\"No articles retrieved.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def search_engine_news(query, num_results=100):\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    results = []\n",
        "    start_index = 1\n",
        "    while len(results) < num_results:\n",
        "        params = {\n",
        "            \"key\": API_KEY_GOOGLE,\n",
        "            \"cx\": CSE_ID,\n",
        "            \"q\": query,\n",
        "            \"num\": 10,\n",
        "            \"start\": start_index,\n",
        "            \"sort\": \"date\"\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            items = data.get(\"items\", [])\n",
        "            if not items:\n",
        "                break\n",
        "            results.extend(items)\n",
        "            start_index += 10\n",
        "            time.sleep(1)\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "            break\n",
        "    return results[:num_results]\n",
        "\n",
        "\n",
        "def filter_search_engine_news(results):\n",
        "    filtered_results = []\n",
        "    for item in results:\n",
        "        title = item.get(\"title\", \"No title\")\n",
        "        link = item.get(\"link\", \"No link\")\n",
        "        snippet = item.get(\"snippet\", \"No content\")\n",
        "        publisher = item.get(\"displayLink\", \"Unknown publisher\")\n",
        "        filtered_results.append({\n",
        "            \"title\": title,\n",
        "            \"url\": link,\n",
        "            \"content\": snippet,\n",
        "            \"publisher\": publisher,\n",
        "            \"source\": publisher,\n",
        "            \"date\": \"Unknown date\"\n",
        "        })\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "def fetch_newsapi_articles(api_key, query, page_size=100, max_results=100):\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    all_articles = []\n",
        "    page = 1\n",
        "    while len(all_articles) < max_results:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"apiKey\": api_key\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            articles = data.get('articles', [])\n",
        "            if not articles:\n",
        "                break\n",
        "            all_articles.extend(articles)\n",
        "            page += 1\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            break\n",
        "    return all_articles\n",
        "\n",
        "\n",
        "media_stack_data = fetch_media_stack_articles()\n",
        "media_stack = filter_mediastack_articles(media_stack_data)\n",
        "if media_stack:\n",
        "    df_media_stack = pd.DataFrame(media_stack)\n",
        "    df_media_stack.to_csv(\"media_stack_filtered_supply_chain_articles.csv\", index=False)\n",
        "\n",
        "query = \"shipment delay\"\n",
        "results = search_engine_news(query)\n",
        "filtered_results = filter_search_engine_news(results)\n",
        "if filtered_results:\n",
        "    df_google = pd.DataFrame(filtered_results)\n",
        "    df_google.to_csv(\"custom_search_engine_news.csv\", index=False)\n",
        "\n",
        "query = \"disruption\"\n",
        "all_articles = fetch_newsapi_articles(API_KEY_NEWSAPI, query)\n",
        "if all_articles:\n",
        "    df_newsapi = pd.DataFrame(all_articles)\n",
        "    df_newsapi.drop(columns=[\"urlToImage\", \"source\"], inplace=True, errors='ignore')\n",
        "    df_newsapi.rename(columns={\"publishedAt\": \"source\"}, inplace=True)\n",
        "    df_newsapi.to_csv(\"newsapi_articles.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "lnYe0gNccACY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXZ9gLlOUTUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}